{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Prepara o spark e java, encontra o spark, cria contexto\n",
    "modifique para o seu caso..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "java version \"1.8.0_202\"\n",
      "Java(TM) SE Runtime Environment (build 1.8.0_202-b08)\n",
      "Java HotSpot(TM) 64-Bit Server VM (build 25.202-b08, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "import os;\n",
    "\n",
    "\n",
    "# os.environ[\"SPARK_HOME\"] = \"Users/guibs/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/pyspark\"\n",
    "# os.environ[\"JAVA_HOME\"] =\"Program Files/Java/jre1.8.0_202\"\n",
    "\n",
    "#os.environ[\"SPARK_HOME\"] = \"/Users/pedro/servers/spark-3.1.1-bin-hadoop2.7\"\n",
    "#os.environ[\"JAVA_HOME\"]=\"/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home\"\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepara o pyspark e spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import findspark \n",
    "\n",
    "# findspark.init()\n",
    "\n",
    "import pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Spark Version  3.1.3\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "print (\"Running Spark Version \", spark.version)\n",
    "\n",
    "#OLD deprecated:\n",
    "#from pyspark import SparkContext\n",
    "\n",
    "#sc = SparkContext('local') \n",
    "#print (\"Running Spark Version \", sc.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTA: EU tive de resolver um erro estranho, qdo corria no Mac OS sem rede:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NOTA - ERROR:\n",
    "    \n",
    "    Spark fails to start in local mode when disconnected [Possible bug in handling IPv6 in Spark??]\n",
    "    \n",
    " am not sure if this will help you, but it solved my problem on Mac.\n",
    "\n",
    "1) Get your hostname. (In terminal, this is usually the first part of the line (before the @ in Linux, before the : in Mac)) (In Mac, you can also type hostname in terminal to get your hostname)\n",
    "\n",
    "2) In /etc/hosts add:\n",
    "\n",
    "127.0.0.1 whatever-your-hostname-is\n",
    "\n",
    "For me, I originally had\n",
    "\n",
    "127.0.0.1 localhost\n",
    "\n",
    "but I changed it to\n",
    "\n",
    "127.0.0.1 my-hostname\n",
    "\n",
    "Save this change and retry pyspark.\n",
    "\n",
    "O que fiz:\n",
    "Macs-MacBook-Pro-4:~ pedro$ hostname\n",
    "Macs-MacBook-Pro-4.local\n",
    "\n",
    "Macs-MacBook-Pro-4:spark-3.2.1-bin-hadoop3.2 pedro$ cp /etc/hosts /etc/hostsBKUP\n",
    "cp: /etc/hostsBKUP: Permission denied\n",
    "Macs-MacBook-Pro-4:spark-3.2.1-bin-hadoop3.2 pedro$ sudo pico /etc/hosts\n",
    "\n",
    "O hosts estava:\n",
    "127.0.0.1\tlocalhost\n",
    "255.255.255.255\tbroadcasthost\n",
    "::1         localhost\n",
    "\n",
    "O hosts ficou:\n",
    "127.0.0.1\tMacs-MacBook-Pro-4.local\n",
    "255.255.255.255\tbroadcasthost\n",
    "::1         Macs-MacBook-Pro-4.local\n",
    "\n",
    "(nota: nao esquecr de repor localhost mais tarde, pode depois falhar com outras appls (?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuacao"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ver o SPark a correr no endereco: http://localhost:4040/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.app.name=pyspark-shell\n",
      "spark.master=local[*]\n",
      "spark.submit.deployMode=client\n",
      "spark.submit.pyFiles=\n",
      "spark.ui.showConsoleProgress=true\n"
     ]
    }
   ],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "conf = SparkConf()\n",
    "print (conf.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1- My first pyspark app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does the next code do? \n",
    "\n",
    "# what changes if you increase the number of samples?\n",
    "# why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.08\n"
     ]
    }
   ],
   "source": [
    "# useful to have this code snippet to avoid getting an error in case forgeting\n",
    "# to close spark\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Using findspark to find automatically the spark folder\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# import python libraries\n",
    "import random\n",
    "\n",
    "# initialize\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "num_samples = 1000\n",
    "#num_samples = 100000000\n",
    "\n",
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "count = spark.sparkContext.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1- SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets do some basic SQL with tables emp and dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- ndep: string (nullable = true)\n",
      "\n",
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "| satish|\n",
      "|krishna|\n",
      "|  amith|\n",
      "|  javed|\n",
      "| prudvi|\n",
      "|   arya|\n",
      "|    joy|\n",
      "|   jack|\n",
      "|  brown|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees = spark.read.json('C:/Users/guibs/Documents/GitHub/SGD/Labs/lab9_pyspark/employee.json')\n",
    "\n",
    "# Print the schema in a tree format\n",
    "\n",
    "employees.printSchema()\n",
    "\n",
    "employees.select(\"name\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dname: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- ndep: string (nullable = true)\n",
      "\n",
      "+----------+--------+----+\n",
      "|     dname|location|ndep|\n",
      "+----------+--------+----+\n",
      "|     SALES| Coimbra|   1|\n",
      "| MARKETING| Coimbra|   2|\n",
      "| LOGISTICS|  Lisbon|   3|\n",
      "|MANAGEMENT|   Porto|   4|\n",
      "+----------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dep = spark.read.json('C:/Users/guibs/Documents/GitHub/SGD/Labs/lab9_pyspark/dep.json')\n",
    "# Print the schema in a tree format\n",
    "\n",
    "dep.printSchema()\n",
    "\n",
    "dep.select(\"*\").show(20)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#OLD DEPRECATED:\n",
    "#dep.registerTempTable(\"dep\")\n",
    "#employees.registerTempTable(\"employees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep.createOrReplaceTempView(\"dep\")\n",
    "employees.createOrReplaceTempView(\"employees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write a query to select all employees aged > 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---------+-------+----+\n",
      "|age|  id|      job|   name|ndep|\n",
      "+---+----+---------+-------+----+\n",
      "| 39|1203|LOGISTICS|  amith|   3|\n",
      "| 29|1206|    SALES|   arya|   1|\n",
      "| 28|1202|MARKETING|krishna|   2|\n",
      "| 25|1201|    SALES| satish|   1|\n",
      "+---+----+---------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM employees WHERE age > 23 ORDER BY AGE DESC\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write a query to show number of employees for each age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|count(AGE)|age|\n",
      "+----------+---+\n",
      "|         1| 29|\n",
      "|         1| 28|\n",
      "|         5| 23|\n",
      "|         1| 25|\n",
      "|         1| 39|\n",
      "+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ageGroups = spark.sql(\"SELECT COUNT (AGE),age FROM employees GROUP BY age\")\n",
    "ageGroups.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show all info of departments of each employee together with all the employee info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---------+-------+----+---------+--------+----+\n",
      "|age|  id|      job|   name|ndep|    dname|location|ndep|\n",
      "+---+----+---------+-------+----+---------+--------+----+\n",
      "| 25|1201|    SALES| satish|   1|    SALES| Coimbra|   1|\n",
      "| 28|1202|MARKETING|krishna|   2|MARKETING| Coimbra|   2|\n",
      "| 39|1203|LOGISTICS|  amith|   3|LOGISTICS|  Lisbon|   3|\n",
      "| 23|1204|    SALES|  javed|   1|    SALES| Coimbra|   1|\n",
      "| 23|1205|    SALES| prudvi|   1|    SALES| Coimbra|   1|\n",
      "| 29|1206|    SALES|   arya|   1|    SALES| Coimbra|   1|\n",
      "| 23|1207|MARKETING|    joy|   2|MARKETING| Coimbra|   2|\n",
      "| 23|1208|MARKETING|   jack|   2|MARKETING| Coimbra|   2|\n",
      "| 23|1209|LOGISTICS|  brown|   3|LOGISTICS|  Lisbon|   3|\n",
      "+---+----+---------+-------+----+---------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.registerTempTable(\"emp\")\n",
    "empdep = spark.sql(\"SELECT * FROM employees,dep WHERE employees.ndep == dep.ndep\")\n",
    "empdep.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same, but restrict to name of employee and of dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+-----+----+-----+--------+----+\n",
      "|age|  id|  job| name|ndep|dname|location|ndep|\n",
      "+---+----+-----+-----+----+-----+--------+----+\n",
      "| 23|1204|SALES|javed|   1|SALES| Coimbra|   1|\n",
      "+---+----+-----+-----+----+-----+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empdep = spark.sql(\"SELECT * FROM employees,dep WHERE employees.ndep == dep.ndep AND dep.dname == 'SALES' AND employees.name = 'javed'\")\n",
    "empdep.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show employee and department name but only for department SALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+----+-----+--------+----+\n",
      "|age|  id|  job|  name|ndep|dname|location|ndep|\n",
      "+---+----+-----+------+----+-----+--------+----+\n",
      "| 25|1201|SALES|satish|   1|SALES| Coimbra|   1|\n",
      "| 23|1204|SALES| javed|   1|SALES| Coimbra|   1|\n",
      "| 23|1205|SALES|prudvi|   1|SALES| Coimbra|   1|\n",
      "| 29|1206|SALES|  arya|   1|SALES| Coimbra|   1|\n",
      "+---+----+-----+------+----+-----+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empVendas = spark.sql(\"SELECT * FROM employees,dep WHERE employees.ndep == dep.ndep AND dep.dname == 'SALES' \")\n",
    "empVendas.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show how many employees there are for each age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|Count|AGE|\n",
      "+-----+---+\n",
      "|    1| 29|\n",
      "|    1| 28|\n",
      "|    5| 23|\n",
      "|    1| 25|\n",
      "|    1| 39|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ageGroups = spark.sql(\"SELECT COUNT(AGE) as Count,AGE FROM employees GROUP BY AGE\")\n",
    "ageGroups.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: string, id: string, job: string, name: string, ndep: string]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another way to read json to view directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---------+-------+----+\n",
      "|age|  id|      job|   name|ndep|\n",
      "+---+----+---------+-------+----+\n",
      "| 25|1201|    SALES| satish|   1|\n",
      "| 28|1202|MARKETING|krishna|   2|\n",
      "| 39|1203|LOGISTICS|  amith|   3|\n",
      "| 23|1204|    SALES|  javed|   1|\n",
      "| 23|1205|    SALES| prudvi|   1|\n",
      "| 29|1206|    SALES|   arya|   1|\n",
      "| 23|1207|MARKETING|    joy|   2|\n",
      "| 23|1208|MARKETING|   jack|   2|\n",
      "| 23|1209|LOGISTICS|  brown|   3|\n",
      "+---+----+---------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE OR REPLACE TEMPORARY VIEW emp1b USING json OPTIONS\" + \n",
    "      \" (path 'C:/Users/guibs/Documents/GitHub/SGD/Labs/lab9_pyspark/employee.json')\")\n",
    "spark.sql(\"select * from emp1b\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing it back to file ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# employees.write.json(\"C:/Users/guibs/Documents/GitHub/SGD/Labs/lab9_pyspark/employees1c.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2 - postgres: now try to write into postgres and see in pgadmin if it worked... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('postgresql://postgres:68296829@localhost:5432/empdep2')\n",
    "employees.toPandas().to_sql('emp', engine)\n",
    "dep.toPandas().to_sql('dep', engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now read from postgres and show contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dbConnection = engine.connect();\n",
    "\n",
    "emp2pd = pd.read_sql(\"select * from \\\"emp\\\"\", dbConnection);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----+---------+-------+----+\n",
      "|index|age|  id|      job|   name|ndep|\n",
      "+-----+---+----+---------+-------+----+\n",
      "|    0| 25|1201|    SALES| satish|   1|\n",
      "|    1| 28|1202|MARKETING|krishna|   2|\n",
      "|    2| 39|1203|LOGISTICS|  amith|   3|\n",
      "|    3| 23|1204|    SALES|  javed|   1|\n",
      "|    4| 23|1205|    SALES| prudvi|   1|\n",
      "|    5| 29|1206|    SALES|   arya|   1|\n",
      "|    6| 23|1207|MARKETING|    joy|   2|\n",
      "|    7| 23|1208|MARKETING|   jack|   2|\n",
      "|    8| 23|1209|LOGISTICS|  brown|   3|\n",
      "+-----+---+----+---------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp2=spark.createDataFrame(emp2pd) \n",
    "emp2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There are some difficulties currently with data types. Try to read from postgres and see employees whose age is larger than 25... does it work? why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ProgrammingError",
     "evalue": "(psycopg2.errors.UndefinedFunction) operator does not exist: text > integer\nLINE 1: select * from \"emp\" where age > 25\n                                      ^\nHINT:  No operator matches the given name and argument types. You might need to add explicit type casts.\n\n[SQL: select * from \"emp\" where age > 25]\n(Background on this error at: https://sqlalche.me/e/14/f405)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUndefinedFunction\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sqlalchemy\\engine\\base.py:1819\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[1;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[0;32m   1818\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[1;32m-> 1819\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1820\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[0;32m   1821\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1823\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sqlalchemy\\engine\\default.py:732\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[1;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 732\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mUndefinedFunction\u001b[0m: operator does not exist: text > integer\nLINE 1: select * from \"emp\" where age > 25\n                                      ^\nHINT:  No operator matches the given name and argument types. You might need to add explicit type casts.\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#DOES NOT WORK, BECAUSE COLUMNS ARE TEXT:\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m empABOVE25 \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mselect * from \u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43memp\u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43m where age > 25\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdbConnection\u001b[49m\u001b[43m)\u001b[49m);\n\u001b[0;32m      5\u001b[0m empABOVE25\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:592\u001b[0m, in \u001b[0;36mread_sql\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize)\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pandas_sql\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[0;32m    584\u001b[0m         sql,\n\u001b[0;32m    585\u001b[0m         index_col\u001b[38;5;241m=\u001b[39mindex_col,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    589\u001b[0m         chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m    590\u001b[0m     )\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:1557\u001b[0m, in \u001b[0;36mSQLDatabase.read_query\u001b[1;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype)\u001b[0m\n\u001b[0;32m   1509\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;124;03mRead SQL query into a DataFrame.\u001b[39;00m\n\u001b[0;32m   1511\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1553\u001b[0m \n\u001b[0;32m   1554\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1555\u001b[0m args \u001b[38;5;241m=\u001b[39m _convert_params(sql, params)\n\u001b[1;32m-> 1557\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1558\u001b[0m columns \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m   1560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:1402\u001b[0m, in \u001b[0;36mSQLDatabase.execute\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1400\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1401\u001b[0m     \u001b[38;5;124;03m\"\"\"Simple passthrough to SQLAlchemy connectable\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnectable\u001b[38;5;241m.\u001b[39mexecution_options()\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sqlalchemy\\engine\\base.py:1291\u001b[0m, in \u001b[0;36mConnection.execute\u001b[1;34m(self, statement, *multiparams, **params)\u001b[0m\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(statement, util\u001b[38;5;241m.\u001b[39mstring_types):\n\u001b[0;32m   1283\u001b[0m     util\u001b[38;5;241m.\u001b[39mwarn_deprecated_20(\n\u001b[0;32m   1284\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing a string to Connection.execute() is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated and will be removed in version 2.0.  Use the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1288\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver-level SQL string.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1289\u001b[0m     )\n\u001b[1;32m-> 1291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_exec_driver_sql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmultiparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_EMPTY_EXECUTION_OPTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfuture\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1300\u001b[0m     meth \u001b[38;5;241m=\u001b[39m statement\u001b[38;5;241m.\u001b[39m_execute_on_connection\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sqlalchemy\\engine\\base.py:1595\u001b[0m, in \u001b[0;36mConnection._exec_driver_sql\u001b[1;34m(self, statement, multiparams, params, execution_options, future)\u001b[0m\n\u001b[0;32m   1585\u001b[0m         (\n\u001b[0;32m   1586\u001b[0m             statement,\n\u001b[0;32m   1587\u001b[0m             distilled_params,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1591\u001b[0m             statement, distilled_parameters, execution_options\n\u001b[0;32m   1592\u001b[0m         )\n\u001b[0;32m   1594\u001b[0m dialect \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialect\n\u001b[1;32m-> 1595\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1597\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_ctx_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_statement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1598\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1599\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1601\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1602\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1603\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m future:\n\u001b[0;32m   1606\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sqlalchemy\\engine\\base.py:1862\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[1;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[0;32m   1859\u001b[0m             branched\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m   1861\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1862\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_dbapi_exception\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m        \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[0;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sqlalchemy\\engine\\base.py:2043\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception\u001b[1;34m(self, e, statement, parameters, cursor, context)\u001b[0m\n\u001b[0;32m   2041\u001b[0m     util\u001b[38;5;241m.\u001b[39mraise_(newraise, with_traceback\u001b[38;5;241m=\u001b[39mexc_info[\u001b[38;5;241m2\u001b[39m], from_\u001b[38;5;241m=\u001b[39me)\n\u001b[0;32m   2042\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m should_wrap:\n\u001b[1;32m-> 2043\u001b[0m     \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2044\u001b[0m \u001b[43m        \u001b[49m\u001b[43msqlalchemy_exception\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_traceback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\n\u001b[0;32m   2045\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2046\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2047\u001b[0m     util\u001b[38;5;241m.\u001b[39mraise_(exc_info[\u001b[38;5;241m1\u001b[39m], with_traceback\u001b[38;5;241m=\u001b[39mexc_info[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sqlalchemy\\util\\compat.py:207\u001b[0m, in \u001b[0;36mraise_\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    204\u001b[0m     exception\u001b[38;5;241m.\u001b[39m__cause__ \u001b[38;5;241m=\u001b[39m replace_context\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;66;03m# credit to\u001b[39;00m\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;66;03m# https://cosmicpercolator.com/2016/01/13/exception-leaks-in-python-2-and-3/\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m# as the __traceback__ object creates a cycle\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m exception, replace_context, from_, with_traceback\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sqlalchemy\\engine\\base.py:1819\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[1;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[0;32m   1817\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1818\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[1;32m-> 1819\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1820\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[0;32m   1821\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1823\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n\u001b[0;32m   1824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mafter_cursor_execute(\n\u001b[0;32m   1825\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1826\u001b[0m         cursor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1830\u001b[0m         context\u001b[38;5;241m.\u001b[39mexecutemany,\n\u001b[0;32m   1831\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sqlalchemy\\engine\\default.py:732\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[1;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 732\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mProgrammingError\u001b[0m: (psycopg2.errors.UndefinedFunction) operator does not exist: text > integer\nLINE 1: select * from \"emp\" where age > 25\n                                      ^\nHINT:  No operator matches the given name and argument types. You might need to add explicit type casts.\n\n[SQL: select * from \"emp\" where age > 25]\n(Background on this error at: https://sqlalche.me/e/14/f405)"
     ]
    }
   ],
   "source": [
    "#DOES NOT WORK, BECAUSE COLUMNS ARE TEXT:\n",
    "\n",
    "empABOVE25 = spark.createDataFrame(pd.read_sql(\"select * from \\\"emp\\\" where age > 25\", dbConnection));\n",
    "\n",
    "empABOVE25.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(age,StringType,true),StructField(id,StringType,true),StructField(job,StringType,true),StructField(name,StringType,true),StructField(ndep,StringType,true)))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#that's because it is defined as text...\n",
    "employees.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look in pgadmin for the type of the columns of the table... is there something wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Even if you specify a schema when you read the json file, it still does not work ... why is the following not working?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- ndep: integer (nullable = true)\n",
      "\n",
      "+----+----+---------+-------+----+\n",
      "| age|  id|      job|   name|ndep|\n",
      "+----+----+---------+-------+----+\n",
      "|null|null|    SALES| satish|null|\n",
      "|null|null|MARKETING|krishna|null|\n",
      "|null|null|LOGISTICS|  amith|null|\n",
      "|null|null|    SALES|  javed|null|\n",
      "|null|null|    SALES| prudvi|null|\n",
      "|null|null|    SALES|   arya|null|\n",
      "|null|null|MARKETING|    joy|null|\n",
      "|null|null|MARKETING|   jack|null|\n",
      "|null|null|LOGISTICS|  brown|null|\n",
      "+----+----+---------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Define custom schema\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schemaemp = StructType([StructField(\"age\", IntegerType(), True), \n",
    "                        StructField(\"id\", LongType(), True),\n",
    "                        StructField(\"job\", StringType(), True),\n",
    "                        StructField(\"name\", StringType(), True),\n",
    "                        StructField(\"ndep\", IntegerType(), True)])\n",
    "\n",
    "emp_with_schema = spark.read.schema(schemaemp) \\\n",
    "        .json(\"C:/Users/guibs/Documents/GitHub/SGD/Labs/lab9_pyspark/employee.json\")\n",
    "\n",
    "emp_with_schema.printSchema()\n",
    "emp_with_schema.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schemas, data types: now create employeesTYPES.json by removing double quotes in non-strings in the json file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The TYPES json file DOES NOT have double quotes in number, such as:\"1\"\n",
    "    \n",
    "    \n",
    "    {\"id\" : \"1201\", \"name\" : \"satish\", \"age\" : \"25\",\"job\":\"SALES\",\"ndep\": \"1\"}}\n",
    "    \n",
    "    into\n",
    "    \n",
    "    {\"id\" : 1201, \"name\" : \"satish\", \"age\" : 25,\"job\":\"SALES\",\"ndep\": 1}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Did this solve the problem with data types?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(age,LongType,true),StructField(id,LongType,true),StructField(job,StringType,true),StructField(name,StringType,true),StructField(ndep,LongType,true)))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesTYPED = spark.read.json('C:/Users/guibs/Documents/GitHub/SGD/Labs/lab9_pyspark/employeeParsed.json')\n",
    "employeesTYPED.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# and we can specify the schema when loading..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- ndep: integer (nullable = true)\n",
      "\n",
      "+---+----+---------+-------+----+\n",
      "|age|  id|      job|   name|ndep|\n",
      "+---+----+---------+-------+----+\n",
      "| 25|1201|    SALES| satish|   1|\n",
      "| 28|1202|MARKETING|krishna|   2|\n",
      "| 39|1203|LOGISTICS|  amith|   3|\n",
      "| 23|1204|    SALES|  javed|   1|\n",
      "| 23|1205|    SALES| prudvi|   1|\n",
      "| 29|1206|    SALES|   arya|   1|\n",
      "| 23|1207|MARKETING|    joy|   2|\n",
      "| 23|1208|MARKETING|   jack|   2|\n",
      "| 23|1209|LOGISTICS|  brown|   3|\n",
      "+---+----+---------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_with_schema = spark.read.schema(schemaemp) \\\n",
    "        .json(\"/Users/guibs/Documents/GitHub/SGD/Labs/lab9_pyspark/employeeParsed.json\")\n",
    "\n",
    "emp_with_schema.printSchema()\n",
    "emp_with_schema.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now write the new typed dataset into a new table emp2. See in pgadmin if the data types are ok now...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesTYPED.toPandas().to_sql('emp2', engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally, redo the query for age above 25 and show that it works...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----+---------+-------+----+\n",
      "|index|age|  id|      job|   name|ndep|\n",
      "+-----+---+----+---------+-------+----+\n",
      "|    1| 28|1202|MARKETING|krishna|   2|\n",
      "|    2| 39|1203|LOGISTICS|  amith|   3|\n",
      "|    5| 29|1206|    SALES|   arya|   1|\n",
      "+-----+---+----+---------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empABOVE25 = spark.createDataFrame(pd.read_sql(\"select * from \\\"emp2\\\" where age > 25\", dbConnection));\n",
    "\n",
    "empABOVE25.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about if I read the original employee with strings but replace the contents of columns with cast? Does it work? why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---------+-------+----+\n",
      "|age|  id|      job|   name|ndep|\n",
      "+---+----+---------+-------+----+\n",
      "| 25|1201|    SALES| satish|   1|\n",
      "| 28|1202|MARKETING|krishna|   2|\n",
      "| 39|1203|LOGISTICS|  amith|   3|\n",
      "| 23|1204|    SALES|  javed|   1|\n",
      "| 23|1205|    SALES| prudvi|   1|\n",
      "| 29|1206|    SALES|   arya|   1|\n",
      "| 23|1207|MARKETING|    joy|   2|\n",
      "| 23|1208|MARKETING|   jack|   2|\n",
      "| 23|1209|LOGISTICS|  brown|   3|\n",
      "+---+----+---------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp3 = spark.read.json(path=\"/Users/guibs/Documents/GitHub/SGD/Labs/lab9_pyspark/employee.json\")\n",
    "emp3.withColumn(\"age\", emp3[\"age\"].cast(\"integer\"))\\\n",
    "  .withColumn(\"id\", emp3[\"id\"].cast(\"long\"))\\\n",
    "  .withColumn(\"ndep\", emp3[\"ndep\"].cast(\"integer\"))\n",
    "\n",
    "emp3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(age,StringType,true),StructField(id,StringType,true),StructField(job,StringType,true),StructField(name,StringType,true),StructField(ndep,StringType,true)))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I think THE SCHEMA TYPES ARE STILL INCORRECT....\n",
    "emp3.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now use the schema but also replace the field contents using casts. Did this work? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---------+-------+----+\n",
      "|age|  id|      job|   name|ndep|\n",
      "+---+----+---------+-------+----+\n",
      "| 25|1201|    SALES| satish|   1|\n",
      "| 28|1202|MARKETING|krishna|   2|\n",
      "| 39|1203|LOGISTICS|  amith|   3|\n",
      "| 23|1204|    SALES|  javed|   1|\n",
      "| 23|1205|    SALES| prudvi|   1|\n",
      "| 29|1206|    SALES|   arya|   1|\n",
      "| 23|1207|MARKETING|    joy|   2|\n",
      "| 23|1208|MARKETING|   jack|   2|\n",
      "| 23|1209|LOGISTICS|  brown|   3|\n",
      "+---+----+---------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#now correct IT USING SCHEMA...\n",
    "\n",
    "#emp3 = spark.read.schema(schemaemp).json(path=\"/Users/pedro/Documents/Aulas/SGD/2022SGD/pratica/aula9_pySpark/employeeTYPES.json\")\n",
    "emp3 = spark.read.json(path=\"/Users/guibs/Documents/GitHub/SGD/Labs/lab9_pyspark/employee.json\")\n",
    "\n",
    "emp3b=emp3.withColumn(\"age\", emp3[\"age\"].cast(\"integer\"))\\\n",
    "  .withColumn(\"id\", emp3[\"id\"].cast(\"long\"))\\\n",
    "  .withColumn(\"ndep\", emp3[\"ndep\"].cast(\"integer\"))\n",
    "\n",
    "emp3b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(age,IntegerType,true),StructField(id,LongType,true),StructField(job,StringType,true),StructField(name,StringType,true),StructField(ndep,IntegerType,true)))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp3b.schema"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
