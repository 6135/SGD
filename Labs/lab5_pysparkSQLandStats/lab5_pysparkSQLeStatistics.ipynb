{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Prepara o spark e java, encontra o spark, cria contexto\n",
    "tente procurar instruções como instalar o SW, dependendo do sistema operativo. De seguida, no Mac OS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"15.0.2\" 2021-01-19\r\n",
      "OpenJDK Runtime Environment AdoptOpenJDK (build 15.0.2+7)\r\n",
      "OpenJDK 64-Bit Server VM AdoptOpenJDK (build 15.0.2+7, mixed mode, sharing)\r\n"
     ]
    }
   ],
   "source": [
    "import os;\n",
    "\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/Users/pedro/servers/spark/spark-3.2.1-bin-hadoop3.2\"\n",
    "os.environ[\"JAVA_HOME\"] =\"/Library/Java/JavaVirtualMachines/adoptopenjdk-15.jdk/Contents/Home\"\n",
    "\n",
    "#os.environ[\"SPARK_HOME\"] = \"/Users/pedro/servers/spark-3.1.1-bin-hadoop2.7\"\n",
    "#os.environ[\"JAVA_HOME\"]=\"/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home\"\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepara o pyspark e spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark \n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Spark Version  3.2.1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "print (\"Running Spark Version \", spark.version)\n",
    "\n",
    "#OLD deprecated:\n",
    "#from pyspark import SparkContext\n",
    "\n",
    "#sc = SparkContext('local') \n",
    "#print (\"Running Spark Version \", sc.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tive de resolver um erro, a correr no Mac OS sem rede:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NOTA - ERROR:\n",
    "    \n",
    "    Spark fails to start in local mode when disconnected [Possible bug in handling IPv6 in Spark??]\n",
    "    \n",
    " am not sure if this will help you, but it solved my problem on Mac.\n",
    "\n",
    "1) Get your hostname. (In terminal, this is usually the first part of the line (before the @ in Linux, before the : in Mac)) (In Mac, you can also type hostname in terminal to get your hostname)\n",
    "\n",
    "2) In /etc/hosts add:\n",
    "\n",
    "127.0.0.1 whatever-your-hostname-is\n",
    "\n",
    "For me, I originally had\n",
    "\n",
    "127.0.0.1 localhost\n",
    "\n",
    "but I changed it to\n",
    "\n",
    "127.0.0.1 my-hostname\n",
    "\n",
    "Save this change and retry pyspark.\n",
    "\n",
    "O que fiz:\n",
    "Macs-MacBook-Pro-4:~ pedro$ hostname\n",
    "Macs-MacBook-Pro-4.local\n",
    "\n",
    "Macs-MacBook-Pro-4:spark-3.2.1-bin-hadoop3.2 pedro$ cp /etc/hosts /etc/hostsBKUP\n",
    "cp: /etc/hostsBKUP: Permission denied\n",
    "Macs-MacBook-Pro-4:spark-3.2.1-bin-hadoop3.2 pedro$ sudo pico /etc/hosts\n",
    "\n",
    "O hosts estava:\n",
    "127.0.0.1\tlocalhost\n",
    "255.255.255.255\tbroadcasthost\n",
    "::1         localhost\n",
    "\n",
    "O hosts ficou:\n",
    "127.0.0.1\tMacs-MacBook-Pro-4.local\n",
    "255.255.255.255\tbroadcasthost\n",
    "::1         Macs-MacBook-Pro-4.local\n",
    "\n",
    "(nota: nao esquecr de repor localhost mais tarde, pode depois falhar com outras appls (?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuacao"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ver o SPark a correr no endereco: http://localhost:4040/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "conf = SparkConf()\n",
    "print (conf.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1- My first pyspark app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does the next code do? \n",
    "\n",
    "# what changes if you increase the number of samples?\n",
    "# why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful to have this code snippet to avoid getting an error in case forgeting\n",
    "# to close spark\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Using findspark to find automatically the spark folder\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# import python libraries\n",
    "import random\n",
    "\n",
    "# initialize\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "num_samples = 10\n",
    "#num_samples = 100000000\n",
    "\n",
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "count = spark.sparkContext.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1- SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets also do some basic SQL with tables emp and dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = spark.read.json('/Users/pedro/Documents/Aulas/SGD/2022SGD/pratica/aula5_pySpark/employee.json')\n",
    "\n",
    "# Print the schema in a tree format\n",
    "\n",
    "employees.printSchema()\n",
    "\n",
    "employees.select(\"name\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep = spark.read.json('/Users/pedro/Documents/Aulas/SGD/2020SGD/praticas/lab6_SparkSparkSQL/dep.json')\n",
    "# Print the schema in a tree format\n",
    "\n",
    "dep.printSchema()\n",
    "\n",
    "dep.select(\"*\").show(20)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#OLD DEPRECATED:\n",
    "#dep.registerTempTable(\"dep\")\n",
    "#employees.registerTempTable(\"employees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep.createOrReplaceTempView(\"dep\")\n",
    "employees.createOrReplaceTempView(\"employees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write a query to select all employees aged > 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from employees where age>23\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write a query to show number of employees for each age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ageGroups = spark.sql(\"?\")\n",
    "ageGroups.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show all info of departments of each employee together with all the employee info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employees.registerTempTable(\"emp\")\n",
    "empdep = spark.sql(\"?\")\n",
    "empdep.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same, but restrict to name of employee and of dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empdep = spark.sql(\"?\")\n",
    "empdep.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally, show employee and department name but only for department SALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empVendas = spark.sql(\"?\")\n",
    "empVendas.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2 - Working with data and statistics in pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read cars dataset, show nr of rows, col names and first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "\n",
    "sc = SparkContext.getOrCreate();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory  /Users/pedro/Documents/Aulas/SGD/2022SGD/pratica/aula5_pySpark\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#default_path=\"/Users/pedro/Documents/Aulas/SGD/2019SGD/praticas/empreparacao/car-data\"\n",
    "#os.chdir(default_path)\n",
    "print(\"Current Working Directory \" , os.getcwd())\n",
    "\n",
    "inp_file = sc.textFile(\"car-data/car-milage.csv\")\n",
    "car_rddText = inp_file.map(lambda line: line.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_rddText.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mpg', 'displacement', 'hp', 'torque', 'CRatio', 'RARatio', 'CarbBarrells', 'NoOfSpeed', 'length', 'width', 'weight', 'automatic']\n"
     ]
    }
   ],
   "source": [
    "titles=car_rddText.first()\n",
    "print(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_file = sc.textFile(\"car-data/car-milage-no-hdr.csv\")\n",
    "car_rdd = inp_file.map(lambda line: line.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(car_rdd.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# what are we doing differently in the next code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from array import array\n",
    "\n",
    "inp_file = sc.textFile(\"car-data/car-milage-no-hdr.csv\")\n",
    "car_rdd = inp_file.map(lambda line: [float(x) for x in line.split(',')])\n",
    "car_rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mean miles per galon?\n",
    "# Max & min Horse Power?\n",
    "# Variance of cars weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "summary = Statistics.colStats(car_rdd)\n",
    "summary.count()\n",
    "print(*titles)\n",
    "print('mean/min/max/var')\n",
    "summary.mean(),summary.min(),summary.max(), summary.variance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What do the following correlations say about the attrs? What about the negative values? \n",
    "hp com weight?\n",
    "ra_ratio com width?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = car_rdd.map(lambda x: x[2])\n",
    "weight = car_rdd.map(lambda x: x[10])\n",
    "print ('%2.3f' % Statistics.corr(hp, weight, method=\"pearson\") )\n",
    "print ('%2.3f' % Statistics.corr(hp, weight, method=\"spearman\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ra_ratio = car_rdd.map(lambda x: x[5])\n",
    "width = car_rdd.map(lambda x: x[9])\n",
    "print ('%2.3f' % Statistics.corr(ra_ratio, width, method=\"pearson\") )\n",
    "print ('%2.3f' % Statistics.corr(ra_ratio, width, method=\"spearman\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recreate the data as a dataframe, show the contents, drop na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cars = pd.read_csv(\"car-data/car-milage.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars=cars.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use describe for statistics. Is there a big or small var in miles per galon?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To continue ...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
